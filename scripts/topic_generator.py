print("[DEBUG] D√©but d'ex√©cution de topic_generator.py")
import os
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import uuid
import requests
from dotenv import load_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
import argparse
from rag_scripts.llm_utils import generate_text
import re

# Chargement des variables d'environnement
load_dotenv()
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_API_KEY = os.getenv("SUPABASE_API_KEY") or os.getenv("SUPABASE_KEY")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
HUGGINGFACE_API_KEY = os.environ.get("HUGGINGFACE_API_KEY")

# Configuration
CHROMA_DIR = "/home/koffi/rag_scripts/chroma_store"
MODEL_PATH = "/home/koffi/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GPTQ/snapshots/6ae1e4ae2cfbaf107c705ed722ec243b4f88014d"

def clean_chunk(text):
    lines = text.split('\n')
    cleaned = []
    for line in lines:
        l = line.strip()
        # Ignore pagination, num√©ros de page, titres g√©n√©riques
        if re.match(r'^(page|chapitre|table des mati√®res|sommaire|contents|index|copyright|isbn|\d+)$', l, re.IGNORECASE):
            continue
        if l.isdigit() or len(l) < 3:
            continue
        cleaned.append(line)
    return '\n'.join(cleaned)

def extract_topics_from_source(source_name, batch_size=8):
    print("[DEBUG] Entr√©e dans extract_topics_from_source")
    print("[DEBUG] Avant HuggingFaceEmbeddings")
    embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
    print("[DEBUG] Apr√®s HuggingFaceEmbeddings")
    print("[DEBUG] Avant Chroma")
    vectorstore = Chroma(persist_directory=CHROMA_DIR, embedding_function=embedding_model)
    print("[DEBUG] Apr√®s Chroma")
    docs = vectorstore.get()["metadatas"]
    texts = vectorstore.get()["documents"]
    print(f"[DEBUG] R√©cup√©ration des metadatas et textes OK : {len(docs)} metadatas, {len(texts)} textes")
    filtered_chunks = [
        texts[i] for i, meta in enumerate(docs)
        if meta.get("source") == source_name
    ]
    print(f"[DEBUG] {len(filtered_chunks)} chunks filtr√©s pour {source_name}")
    if not filtered_chunks:
        print("‚ö†Ô∏è Aucun chunk trouv√© pour ce document.")
        return []
    print("[DEBUG] D√©but de la boucle de g√©n√©ration de topics")
    all_topics = []
    max_prompt_length = 4000
    for i in range(0, len(filtered_chunks), batch_size):
        if i >= batch_size * 5:
            print("[DEBUG] Arr√™t anticip√© pour test (5 batchs)")
            break
        print(f"[DEBUG] Batch {i//batch_size+1} sur {len(filtered_chunks)//batch_size+1}")
        batch = [clean_chunk(chunk) for chunk in filtered_chunks[i:i + batch_size]]
        full_text = "\n".join(batch)
        if len(full_text) > max_prompt_length:
            full_text = full_text[:max_prompt_length]
        prompt = (
            "Voici un extrait d'un livre. Donne une **liste de 10 √† 30 topics importants**, "
            "chaque topic sur une seule ligne. Pas de phrases longues. Pas d'exemples. Pas de d√©tails. "
            "Seulement des mots-cl√©s ou concepts brefs (1 √† 6 mots).\n\n"
            "N'inclus pas la pagination, les num√©ros de page, les titres de chapitre, ou tout √©l√©ment technique. "
            "Ne retiens que les vrais concepts ou id√©es du livre.\n\n"
            f"---\n{full_text}\n---\n\nListe :\n"
        )
        print(f"[INFO] G√©n√©ration de topics pour le batch {i//batch_size+1} (taille: {len(batch)})")
        print(f"[DEBUG] Prompt envoy√© (premiers 200 caract√®res): {prompt[:200]} ...")
        try:
            result = generate_text(prompt, model="deepseek-chat", temperature=0.3, max_tokens=300)
            print("[INFO] R√©ponse re√ßue de DeepSeek.")
        except Exception as e:
            print(f"[ERROR] Erreur lors de l'appel √† DeepSeek: {e}")
            continue
        lines = result.split("Liste :")[-1].strip().split("\n")
        raw_topics = [line.strip("-‚Ä¢*‚Äì‚Äî1234567890. ").strip() for line in lines if line.strip()]
        # Post-filtrage : ignore topics qui ressemblent √† des num√©ros, √† 'Page X', ou trop courts
        filtered = [
            t for t in raw_topics
            if 3 <= len(t) <= 60 and not re.match(r'^(page|chapitre|\d+)$', t, re.IGNORECASE) and t.lower() not in ("liste", "introduction") and " " in t
        ]
        all_topics.extend(filtered)

    unique = list(dict.fromkeys(all_topics))
    print(f"‚úÖ {len(unique)} topics nettoy√©s.")
    return unique

def insert_topics_to_supabase(topics, user_id, source_name):
    print("üì§ Insertion dans Supabase via REST API...")
    headers = {
        "apikey": SUPABASE_API_KEY,
        "Authorization": f"Bearer {SUPABASE_API_KEY}",
        "Content-Type": "application/json"
    }

    success = 0
    for topic in topics:
        payload = {
            "id": str(uuid.uuid4()),
            "name": topic,
            "label": topic,
            "source": source_name,
            "user_id": user_id
        }
        response = requests.post(
            f"{SUPABASE_URL}/rest/v1/topics",
            headers=headers,
            json=payload
        )
        if response.status_code in (200, 201):
            success += 1
        else:
            print(f"‚ùå Erreur pour '{topic}' : {response.status_code} {response.text}")

    print(f"‚úÖ {success} topics ins√©r√©s avec succ√®s.")

if __name__ == "__main__":
    print("[DEBUG] Entr√©e dans le main de topic_generator.py")
    parser = argparse.ArgumentParser()
    parser.add_argument("source_name", nargs="?", type=str, help="Nom du PDF √† traiter (ex: BeYourOwnSailingCoach.pdf)")
    parser.add_argument("--user_id", type=str, default=None, help="ID utilisateur pour l'insertion Supabase (UUID)")
    parser.add_argument("--test-deepseek", action="store_true", help="Test rapide de l'API DeepSeek")
    args = parser.parse_args()

    if args.test_deepseek:
        print("[TEST] Appel DeepSeek avec prompt minimal...")
        try:
            result = generate_text("Bonjour, peux-tu g√©n√©rer une liste de 3 topics sur la gestion du temps ?", model="deepseek-chat", temperature=0.3, max_tokens=100)
            print("[SUCCESS] R√©ponse DeepSeek :")
            print(result)
        except Exception as e:
            print(f"[ERROR] DeepSeek API test failed: {e}")
        exit(0)

    # Si aucun user_id n'est fourni, on g√©n√®re un UUID al√©atoire
    user_id = args.user_id if args.user_id else str(uuid.uuid4())
    topics = extract_topics_from_source(args.source_name)
    print("[DEBUG] Fin de extract_topics_from_source")
    if not topics:
        print("‚ö†Ô∏è Aucun topic g√©n√©r√© pour ce document.")
    else:
        insert_topics_to_supabase(topics, user_id, args.source_name)

